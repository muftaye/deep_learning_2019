{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT & TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Latest Data Split - Blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Data/latest/articles_train.csv\")\n",
    "data_test = pd.read_csv(\"Data/latest/articles_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>content_source_desc</th>\n",
       "      <th>content_title_clean</th>\n",
       "      <th>content_body_clean</th>\n",
       "      <th>blind_mean_rating</th>\n",
       "      <th>blind_rating_count</th>\n",
       "      <th>blind_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2932</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>A Tax Cut That Lifts the Economy? Opinions Are...</td>\n",
       "      <td>Yet if the House plan resolves some longstandi...</td>\n",
       "      <td>3.177778</td>\n",
       "      <td>45</td>\n",
       "      <td>[4.5, 1.5, 0.5, 4.5, 1.0, 4.0, 3.5, 3.5, 1.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2870</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Tom Tancredo enters Colorado governor's race, ...</td>\n",
       "      <td>Former U.S. Rep. Tom Tancredo announced Tuesda...</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>16</td>\n",
       "      <td>[3.5, 4.0, 3.0, 2.5, 0.5, 3.0, 0.5, 0.5, 4.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2869</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Panel Recommends Opioid Solutions but Puts No ...</td>\n",
       "      <td>President Trump’s bipartisan commission on th...</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>6</td>\n",
       "      <td>[5.0, 4.0, 4.5, 3.5, 2.0, 4.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2864</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Trump vows to end non merit-base immigration, ...</td>\n",
       "      <td>President Trump vowed Wednesday to scrap the f...</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>[2.0, 2.5, 4.0, 0.5, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2868</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>WATCH: Congress Holds Hearing on Banning Abort...</td>\n",
       "      <td>Congress will hold a hearing Wednesday on a bi...</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>21</td>\n",
       "      <td>[2.0, 1.0, 0.5, 1.0, 3.5, 5.0, 2.0, 0.5, 2.5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content_id  month  day  year        date content_source_desc  \\\n",
       "0        2932     11    2  2017  2017-11-02  The New York Times   \n",
       "1        2870     11    1  2017  2017-11-01            Fox News   \n",
       "2        2869     11    1  2017  2017-11-01  The New York Times   \n",
       "3        2864     11    1  2017  2017-11-01            Fox News   \n",
       "4        2868     11    1  2017  2017-11-01           Breitbart   \n",
       "\n",
       "                                 content_title_clean  \\\n",
       "0  A Tax Cut That Lifts the Economy? Opinions Are...   \n",
       "1  Tom Tancredo enters Colorado governor's race, ...   \n",
       "2  Panel Recommends Opioid Solutions but Puts No ...   \n",
       "3  Trump vows to end non merit-base immigration, ...   \n",
       "4  WATCH: Congress Holds Hearing on Banning Abort...   \n",
       "\n",
       "                                  content_body_clean  blind_mean_rating  \\\n",
       "0  Yet if the House plan resolves some longstandi...           3.177778   \n",
       "1  Former U.S. Rep. Tom Tancredo announced Tuesda...           2.375000   \n",
       "2   President Trump’s bipartisan commission on th...           3.916667   \n",
       "3  President Trump vowed Wednesday to scrap the f...           2.100000   \n",
       "4  Congress will hold a hearing Wednesday on a bi...           2.428571   \n",
       "\n",
       "   blind_rating_count                                      blind_ratings  \n",
       "0                  45  [4.5, 1.5, 0.5, 4.5, 1.0, 4.0, 3.5, 3.5, 1.5, ...  \n",
       "1                  16  [3.5, 4.0, 3.0, 2.5, 0.5, 3.0, 0.5, 0.5, 4.5, ...  \n",
       "2                   6                     [5.0, 4.0, 4.5, 3.5, 2.0, 4.5]  \n",
       "3                   5                          [2.0, 2.5, 4.0, 0.5, 1.5]  \n",
       "4                  21  [2.0, 1.0, 0.5, 1.0, 3.5, 5.0, 2.0, 0.5, 2.5, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "def clean_data(text):\n",
    "    #remove punctuation, digits, extra stuff. make lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    #lemma it - include POS tag in order to lemma it better\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    textTokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    word_tokens_nostop = [w for w in textTokens if not w in stopwords.words('english')] \n",
    "    #now lemma\n",
    "    text = [lemmatizer.lemmatize(tok, tag_map[tag[0]]) for tok, tag in pos_tag(word_tokens_nostop)]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clean body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body_train = data_train[\"content_body_clean\"].apply(clean_data)\n",
    "y_train = data_train['blind_mean_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body_test = data_test[\"content_body_clean\"].apply(clean_data)\n",
    "y_test = data_test['blind_mean_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index 29377\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 29000\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(clean_body_train)\n",
    "\n",
    "# Note, the tokenizer's word_index will not respect VOCAB_SIZE.\n",
    "# but, that parameter will be respected in later methods,\n",
    "# (for example, when you call text_to_sequences).\n",
    "# Also note that '0' is a reserved index for padding.\n",
    "print(\"Word index\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_t, y_val = train_test_split(clean_body_train, y_train, test_size=0.15, shuffle=True, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the texts_to_sequences utility to vectorize your training, \n",
    "# validation, and test questions. \n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(clean_body_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Words in the 90 percentile: 764.6\n",
      "# of Words in the 95 percentile: 1100.7999999999997\n",
      "# of Words in the 99 percentile: 1879.800000000003\n",
      "# of Words in the 100 percentile: 9317.0\n"
     ]
    }
   ],
   "source": [
    "train_word_lengths = []\n",
    "for w in sequences_train:\n",
    "    train_word_lengths.append(len(w))\n",
    "words_length = np.array(train_word_lengths)\n",
    "\n",
    "print(\"# of Words in the 90 percentile:\",np.percentile(words_length, 90))\n",
    "print(\"# of Words in the 95 percentile:\",np.percentile(words_length, 95))\n",
    "print(\"# of Words in the 99 percentile:\",np.percentile(words_length, 99))\n",
    "print(\"# of Words in the 100 percentile:\",np.percentile(words_length, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = tf.keras.preprocessing.sequence.pad_sequences(sequences_train, maxlen=MAX_SEQ_LEN)\n",
    "padded_val = tf.keras.preprocessing.sequence.pad_sequences(sequences_val, maxlen=MAX_SEQ_LEN)\n",
    "padded_test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ...,   20, 3553,  415])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.dataset and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will create a dataset that returns four elements.\n",
    "# - a batch of padded body\n",
    "# - a batch of ratings\n",
    "def create_dataset(body, ratings):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((body, \n",
    "                                                ratings))\n",
    "    # Shuffle and batch\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(padded_train, y_train_t)\n",
    "val_ds = create_dataset(padded_val, y_val)\n",
    "test_ds = create_dataset(padded_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_advanced(x):\n",
    "    return K.relu(x, max_value=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "embedded_body = Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=MAX_SEQ_LEN)(body_input)\n",
    "encoded_body = LSTM(64)(embedded_body)\n",
    "\n",
    "# Concatenate other features\n",
    "#merged = tf.keras.layers.concatenate([encoded_image, encoded_question])\n",
    "\n",
    "# dense layers\n",
    "dense1 = Dense(128, activation=\"relu\")(encoded_body)\n",
    "dense2 = Dense(32, activation=\"relu\")(dense1)\n",
    "\n",
    "# Next, add a binary classifier on top\n",
    "output = Dense(1, activation=relu_advanced)(dense2)\n",
    "\n",
    "# Your final model\n",
    "model1 = Model(inputs=body_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 3000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 3000, 64)          1856000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,901,505\n",
      "Trainable params: 1,901,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam', \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to help us record metrics.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate on validation set\n",
    "def evaluate(max_steps=None):\n",
    "    steps = 0\n",
    "    for body_batch, y_batch in val_ds:\n",
    "        if max_steps != None and steps == max_steps:\n",
    "            break\n",
    "        predictions = model1.predict(x=body_batch)\n",
    "        steps += 1 \n",
    "        # Record metrics after each batch\n",
    "        val_loss(y_batch, predictions)\n",
    "        val_accuracy(y_batch, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints folder already exists\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir=\"checkpoints/\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"Checkpoints folder already exists\")\n",
    "else:\n",
    "    print(\"Creating a checkpoints directory\")\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history, train_acc_history = [], []\n",
    "val_loss_history, val_acc_history = [], []\n",
    "\n",
    "epochs = 31 # Your code here\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Train for one epoch\n",
    "    for body_batch, y_batch in train_ds:\n",
    "        result = model1.train_on_batch(x=body_batch, y=y_batch)\n",
    "\n",
    "    # Record metrics after each batch\n",
    "    train_loss(result[0])\n",
    "    train_accuracy(result[1])\n",
    "\n",
    "    # Evaluate for a few steps\n",
    "    evaluate(max_steps=100)\n",
    "\n",
    "    # Print progress\n",
    "    # You should not need to modify this.\n",
    "    template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Val Loss {:.2f}, Val Accuracy {:.2f}, Time: {:.1f} secs'\n",
    "    print(template.format(epoch,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100,\n",
    "                        val_loss.result(),\n",
    "                        val_accuracy.result() * 100,\n",
    "                        time.time() - start))\n",
    "  \n",
    "    # Record history\n",
    "    train_loss_history.append(train_loss.result())\n",
    "    train_acc_history.append(train_accuracy.result() * 100)\n",
    "    val_loss_history.append(val_loss.result())\n",
    "    val_acc_history.append(val_accuracy.result() * 100)\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    # Your code here\n",
    "    # Save a checkpoint after each epoch\n",
    "    cpNum=\"cp-epoch-\"+str(epoch)+\".ckpt\"\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, cpNum)\n",
    "    print(\"Saving weights\")\n",
    "    model1.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def plot(train_loss_h, train_acc_h, val_loss_h, val_acc_h):\n",
    "    # The history object contains results on the training and test\n",
    "    # sets for each epoch\n",
    "    acc = train_acc_h\n",
    "    val_acc = val_acc_h\n",
    "    loss = train_loss_h\n",
    "    val_loss = val_loss_h\n",
    "\n",
    "    # Get the number of epochs\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.plot(epochs, acc, color='blue', label='Train')\n",
    "    plt.plot(epochs, val_acc, color='orange', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    _ = plt.figure()\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.plot(epochs, loss, color='blue', label='Train')\n",
    "    plt.plot(epochs, val_loss, color='orange', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
